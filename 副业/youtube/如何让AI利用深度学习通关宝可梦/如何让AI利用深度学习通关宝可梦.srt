1
00:00:18,833 --> 00:00:20,766
现在，我们正在观看

2
00:00:20,800 --> 00:00:23,066
AI 玩的 20,000 场游戏，因为它一开始探索

3
00:00:23,066 --> 00:00:24,900
Pokémon

4
00:00:24,900 --> 00:00:27,800
Red 的世界，一开始它没有

5
00:00:27,833 --> 00:00:29,900
任何知识，只能

6
00:00:29,900 --> 00:00:32,533
按随机

7
00:00:32,533 --> 00:00:34,700
按钮，但在 5 年的

8
00:00:34,733 --> 00:00:36,866
模拟游戏时间中，它获得了

9
00:00:36,900 --> 00:00:38,966
通过从

10
00:00:39,000 --> 00:00:41,333
经验中学习，人工智能最终能够

11
00:00:41,333 --> 00:00:43,466
捕捉神奇宝贝并进化它们并击败

12
00:00:43,500 --> 00:00:45,800
健身房领导者，它甚至设法

13
00:00:45,833 --> 00:00:47,833
利用游戏的随机数

14
00:00:47,833 --> 00:00:50,333
生成器，但也许

15
00:00:50,333 --> 00:00:52,166
比它的成功更令人着迷的是它

16
00:00:52,200 --> 00:00:54,500
失败的方式 令人惊讶的是，

17
00:00:54,500 --> 00:00:57,400
与我们自己的人类经历相关，事实

18
00:00:57,433 --> 00:00:58,933
证明，研究算法的行为

19
00:00:58,933 --> 00:01:00,666
实际上可以让我们更多地了解

20
00:01:00,700 --> 00:01:03,533
自己。在这段视频中，我将讲述

21
00:01:03,533 --> 00:01:05,600
这个人工智能的发展故事，并

22
00:01:05,633 --> 00:01:08,266
分析它在最后学习的策略。

23
00:01:08,266 --> 00:01:10,166
深入了解一些技术

24
00:01:10,200 --> 00:01:11,733
细节并向您展示如何自己下载和

25
00:01:11,733 --> 00:01:19,633
运行该程序让

26
00:01:19,666 --> 00:01:23,533
我们首先询问它是如何

27
00:01:23,533 --> 00:01:26,433
工作的人工智能与游戏的交互

28
00:01:26,466 --> 00:01:28,966
就像人类一样它从屏幕上获取图像

29
00:01:28,966 --> 00:01:31,066
并选择要

30
00:01:31,100 --> 00:01:36,933
按下的按钮 使用强化学习来优化它的选择，

31
00:01:36,933 --> 00:01:38,933
通过这种方法，我们不必

32
00:01:38,933 --> 00:01:42,066
明确告诉人工智能要按哪些按钮，

33
00:01:42,100 --> 00:01:43,733
我们只需要向它提供有关游戏

34
00:01:43,733 --> 00:01:45,866
玩得如何的高级反馈，

35
00:01:45,900 --> 00:01:48,500
这意味着如果我们

36
00:01:48,500 --> 00:01:50,933
分配奖励 为了达到我们希望

37
00:01:50,933 --> 00:01:53,433
它完成的目标，人工智能可以

38
00:01:53,466 --> 00:01:56,366
通过反复试验自行学习，所以

39
00:01:56,400 --> 00:01:59,033
这看起来像什么人工智能将在没有任何

40
00:01:59,033 --> 00:02:01,266
知识或技能的情况下开始，并且

41
00:02:01,266 --> 00:02:03,566
只能按随机按钮，

42
00:02:03,566 --> 00:02:05,600
以便它获得任何结果 有用的反馈，我们

43
00:02:05,633 --> 00:02:07,566
需要创建一个温和的

44
00:02:07,566 --> 00:02:09,166
奖励课程，引导它

45
00:02:09,200 --> 00:02:11,533
学习困难的目标，

46
00:02:11,533 --> 00:02:13,166
也许最基本的目标

47
00:02:13,200 --> 00:02:15,633
是探索地图，我们

48
00:02:15,633 --> 00:02:17,500
希望有一种方式在 AI

49
00:02:17,500 --> 00:02:20,366
到达新位置时给予奖励，更一般地说，

50
00:02:20,400 --> 00:02:23,200
我们 为了激发好奇心，

51
00:02:23,200 --> 00:02:25,533
实现这一点的一种方法是记录

52
00:02:25,533 --> 00:02:28,333
人工智能在玩游戏时看到的每个屏幕，

53
00:02:28,333 --> 00:02:30,200
我们可以将

54
00:02:30,233 --> 00:02:31,966
当前屏幕与

55
00:02:32,000 --> 00:02:33,700
记录中的所有屏幕进行比较，看看是否有任何

56
00:02:33,700 --> 00:02:36,366
接近的匹配 如果没有找到匹配项，

57
00:02:36,400 --> 00:02:37,733
这意味着人工智能发现了一些

58
00:02:37,733 --> 00:02:39,300
新东西，因此它会给予它

59
00:02:39,333 --> 00:02:41,133
奖励，并将新屏幕添加到

60
00:02:41,133 --> 00:02:43,466
奖励它的记录中，因为独特的屏幕

61
00:02:43,500 --> 00:02:45,000
应该鼓励它立即找到游戏的新部分

62
00:02:45,033 --> 00:02:48,033
并寻找新奇的东西

63
00:02:48,033 --> 00:02:49,766
我们有一个目标 让我们

64
00:02:49,766 --> 00:02:53,666
开始学习过程，看看

65
00:02:53,700 --> 00:02:56,800
这个阶段进展如何 人工智能本质上是

66
00:02:56,833 --> 00:02:58,966
按随机按钮，只是为了看看

67
00:02:59,000 --> 00:03:01,200
会发生什么，以便更快地收集经验

68
00:03:01,200 --> 00:03:03,233
我们将让它同时玩 40 个游戏

69
00:03:03,233 --> 00:03:05,900
，每个游戏玩 2 个小时

70
00:03:05,900 --> 00:03:08,866
[音乐]几个

71
00:03:08,866 --> 00:03:11,533
小时后，人工智能将审查所有

72
00:03:11,533 --> 00:03:13,566
游戏，并根据

73
00:03:13,600 --> 00:03:16,266
其获得的奖励进行自我更新，如果进展顺利，我们

74
00:03:16,266 --> 00:03:19,900
应该会看到渐进式改进，并且

75
00:03:19,900 --> 00:03:22,133
在经过几次迭代

76
00:03:22,133 --> 00:03:24,266
训练后，人工智能会找到出路，整个过程可以重复

77
00:03:24,300 --> 00:03:26,233
起始房间明显比

78
00:03:26,266 --> 00:03:28,733
当它的行为由于某种

79
00:03:28,733 --> 00:03:30,633
原因随机时要快，尽管

80
00:03:30,666 --> 00:03:33,133
它没有探索一号公路，而是专注于

81
00:03:33,133 --> 00:03:36,000
托盘镇的特定区域，为什么会

82
00:03:36,033 --> 00:03:38,366
这样，事实证明，当你寻求

83
00:03:38,400 --> 00:03:41,433
新奇时，很容易分心，

84
00:03:41,433 --> 00:03:43,766
特别是 它被困的区域有

85
00:03:43,766 --> 00:03:46,766
动画水草和

86
00:03:46,766 --> 00:03:49,866
四处走动的 NPC，事实证明这个动画

87
00:03:49,900 --> 00:03:51,666
实际上足以多次触发新奇

88
00:03:51,700 --> 00:03:54,700
奖励，因此根据我们

89
00:03:54,700 --> 00:03:56,933
自己的目标，只是闲逛并

90
00:03:56,933 --> 00:03:59,300
欣赏风景

91
00:03:59,333 --> 00:04:01,233
比探索探索其余部分更有价值

92
00:04:01,266 --> 00:04:03,766
这是我们

93
00:04:03,766 --> 00:04:05,366
在现实生活中遇到的一个悖论，

94
00:04:05,400 --> 00:04:08,133
好奇心引导我们做出最

95
00:04:08,133 --> 00:04:10,233
重要的发现，但

96
00:04:10,266 --> 00:04:12,200
同时它也让我们容易

97
00:04:12,233 --> 00:04:14,900
分心，让我们陷入

98
00:04:14,900 --> 00:04:18,633
麻烦。作为人类，我们可以反思

99
00:04:18,666 --> 00:04:21,666
自己内在动机的来源，但

100
00:04:21,666 --> 00:04:25,000
我们可以 相比之下，改变它们并不容易，

101
00:04:25,033 --> 00:04:26,733
我们很容易改变

102
00:04:26,733 --> 00:04:27,766
这些，让

103
00:04:27,800 --> 00:04:31,200
人工智能返回到我们当前的场景，

104
00:04:31,200 --> 00:04:33,233
如果

105
00:04:33,233 --> 00:04:35,833
与之前看到的屏幕有多个像素不同，就会触发奖励，

106
00:04:35,833 --> 00:04:38,300
但如果我们将此阈值提高

107
00:04:38,300 --> 00:04:40,966
到数百像素，动画就会触发

108
00:04:41,000 --> 00:04:42,833
不足以再触发奖励，

109
00:04:42,833 --> 00:04:45,533
这意味着人工智能将不再

110
00:04:45,533 --> 00:04:47,800
从观看它们中获得任何满足感，

111
00:04:47,833 --> 00:04:49,633
只会对

112
00:04:49,633 --> 00:04:51,133
更新颖的

113
00:04:51,133 --> 00:04:53,400
地点产生兴趣。请注意，每次我们修改

114
00:04:53,433 --> 00:04:55,733
奖励时，我们都会从头开始重新启动人工智能学习，

115
00:04:55,733 --> 00:05:00,400
因为我们 希望在

116
00:05:00,433 --> 00:05:02,366
这次更改后可以重现整个过程，人工智能开始

117
00:05:02,366 --> 00:05:05,266
探索路线

118
00:05:05,300 --> 00:05:25,700
[音乐]

119
00:05:25,700 --> 00:05:28,566
一条，最终它到

120
00:05:28,600 --> 00:05:30,333
达维迪安城，

121
00:05:30,333 --> 00:05:32,833
这是巨大的进步，但现在

122
00:05:32,866 --> 00:05:35,633
在大多数战斗中还有另一个问题，

123
00:05:35,666 --> 00:05:37,933
屏幕看起来几乎相同，所以

124
00:05:37,933 --> 00:05:39,766
有

125
00:05:39,800 --> 00:05:42,733
在战斗中获得的探索奖励并不多，这反过来会

126
00:05:42,733 --> 00:05:45,166
导致人工智能简单地逃离

127
00:05:45,200 --> 00:05:47,933
它们，但最终它无法在

128
00:05:47,933 --> 00:05:50,333
不战斗的情况下取得进展，因此为了解决这个问题，

129
00:05:50,333 --> 00:05:54,133
让我们根据现在所有神奇宝贝的综合水平添加额外的奖励

130
00:05:54,133 --> 00:05:56,466
这种

131
00:05:56,500 --> 00:05:58,500
获得水平的新激励人工智能逐渐开始

132
00:05:58,500 --> 00:06:00,766
赢得战斗猫捕捉神奇宝贝并

133
00:06:00,800 --> 00:06:03,833
升级它们最终神奇宝贝

134
00:06:03,833 --> 00:06:05,266
达到足够高的水平，他们

135
00:06:05,266 --> 00:06:07,733
开始进化它实际上倾向于

136
00:06:07,733 --> 00:06:09,766
首先取消进化，然后

137
00:06:09,766 --> 00:06:12,066
最终决定它是

138
00:06:12,100 --> 00:06:14,333
有益的然而它确实如此

139
00:06:14,333 --> 00:06:18,966
当神奇宝贝战斗足够长的时间使其默认动作耗尽时，似乎会陷入困境，

140
00:06:19,000 --> 00:06:22,933
尽管经过版本 45 的更多训练迭代后，

141
00:06:22,933 --> 00:06:25,633
在这一点上有了实质性的改进，我们

142
00:06:25,666 --> 00:06:27,333
现在第一次看到比比鸟正在进化，

143
00:06:27,333 --> 00:06:29,000
它也终于

144
00:06:29,033 --> 00:06:30,533
弄清楚了 当一个动作

145
00:06:30,533 --> 00:06:32,400
耗尽并且能够切换

146
00:06:32,433 --> 00:06:34,633
到替代动作时该怎么办，这

147
00:06:34,633 --> 00:06:36,433
对于

148
00:06:36,433 --> 00:06:39,866
稍后在版本 60 中发生的事情非常重要，AI

149
00:06:39,866 --> 00:06:42,000
开始进入 vidian 森林并开始

150
00:06:42,033 --> 00:06:43,800
其第一个训练师

151
00:06:43,800 --> 00:06:47,666
[音乐]

152
00:06:47,700 --> 00:06:49,933
战斗 至此，它实际上已经有了

153
00:06:49,933 --> 00:06:51,800
足够的经验，它在

154
00:06:51,833 --> 00:06:54,000
第一次

155
00:06:54,000 --> 00:06:58,566
[音乐]

156
00:06:58,600 --> 00:07:02,766
遭遇中就取得了成功，

157
00:07:02,766 --> 00:07:04,700
此后它慢慢地开始弄清楚

158
00:07:04,733 --> 00:07:06,666
如何导航

159
00:07:06,700 --> 00:07:28,466
[音乐]

160
00:07:28,500 --> 00:07:29,533
森林

161
00:07:29,566 --> 00:07:54,766
[音乐]，

162
00:07:54,766 --> 00:07:57,433
最终在版本65它找到了

163
00:07:57,466 --> 00:07:59,200
如何穿过森林并

164
00:07:59,233 --> 00:08:06,033
前往 P

165
00:08:06,033 --> 00:08:09,733
城，但尽管

166
00:08:09,733 --> 00:08:12,066
取得了很大进展，但仍然有些不对劲，AI

167
00:08:12,100 --> 00:08:14,233
会直接投入战斗，即使

168
00:08:14,266 --> 00:08:17,100
它无法获胜，这会让事情变得

169
00:08:17,100 --> 00:08:19,433
更糟，它永远不会访问神奇宝贝中心

170
00:08:19,466 --> 00:08:21,766
来治愈，这意味着当 它输了，

171
00:08:21,766 --> 00:08:23,800
一直回到

172
00:08:23,833 --> 00:08:26,466
游戏的一开始，我们可以尝试通过在

173
00:08:26,466 --> 00:08:28,466
输掉战斗时减去奖励来解决这个问题，

174
00:08:28,466 --> 00:08:30,566
但这并不

175
00:08:30,600 --> 00:08:33,266
像我们希望的那样有效，而不是在

176
00:08:33,266 --> 00:08:34,933
即将输掉时避免困难的战斗

177
00:08:34,933 --> 00:08:37,600
人工智能只是拒绝按下

178
00:08:37,633 --> 00:08:40,166
按钮来

179
00:08:40,200 --> 00:08:42,700
无限期地继续拖延，这在技术上满足了

180
00:08:42,700 --> 00:08:45,400
目标，但这不是我们

181
00:08:45,433 --> 00:08:46,966
在研究其影响时的意图，

182
00:08:47,000 --> 00:08:48,866
但是我们注意到，在极少数

183
00:08:48,866 --> 00:08:53,600
情况下，还有其他东西

184
00:08:53,633 --> 00:08:55,266
每次只减去一次大量奖励 在训练运行中，

185
00:08:55,266 --> 00:08:57,200
始终有一个游戏会扣除

186
00:08:57,233 --> 00:09:02,366
比我们预期大 10 倍的奖励，在

187
00:09:02,400 --> 00:09:04,500
这种情况发生之前重播，我们看到

188
00:09:04,500 --> 00:09:09,433
AI 走进神奇宝贝中心，在

189
00:09:09,466 --> 00:09:11,366
登录后漫步到角落里的计算机并漫无目的地按下

190
00:09:11,400 --> 00:09:13,800
按钮一段时间后，它会将

191
00:09:13,800 --> 00:09:16,100
Pokemon 存入系统，并立即

192
00:09:16,100 --> 00:09:18,766
丢失大量奖励，这是

193
00:09:18,800 --> 00:09:21,100
因为奖励被分配为

194
00:09:21,100 --> 00:09:23,500
Pokémon 等级的总和，因此存入

195
00:09:23,500 --> 00:09:27,300
13 级 Pokémon 会立即失去 13 级，

196
00:09:27,300 --> 00:09:29,666
这会发出如此强烈的

197
00:09:29,700 --> 00:09:31,733
负面影响 表明它实际上会给人工智能带来

198
00:09:31,733 --> 00:09:37,766
类似创伤性的经历，

199
00:09:37,800 --> 00:09:40,400
它不像人类那样有情感，

200
00:09:40,400 --> 00:09:42,433
但具有极端

201
00:09:42,433 --> 00:09:44,333
奖励值的单个​​事件仍然可以对其行为产生持久的

202
00:09:44,333 --> 00:09:46,866
影响，在这种情况下，

203
00:09:46,900 --> 00:09:51,166
它只失去了一次神奇宝贝 足以

204
00:09:51,200 --> 00:09:53,133
与整个神奇宝贝中心形成负面关联，并且人工智能

205
00:09:53,133 --> 00:09:55,333
将在所有未来的游戏中完全避免它

206
00:09:55,333 --> 00:09:57,966
根本原因是我们从未

207
00:09:58,000 --> 00:10:00,000
考虑到

208
00:10:00,000 --> 00:10:02,500
总级别可能会降低的意外情况，以

209
00:10:02,500 --> 00:10:04,400
解决此问题我们将修改奖励

210
00:10:04,433 --> 00:10:06,133
功能，以便它只在

211
00:10:06,133 --> 00:10:08,833
级别增加时提供任何奖励这

212
00:10:08,866 --> 00:10:11,200
似乎解决了重新启动训练后的问题

213
00:10:11,233 --> 00:10:13,166
人工智能开始访问

214
00:10:13,200 --> 00:10:16,400
神奇宝贝

215
00:10:16,400 --> 00:10:28,200
[音乐]

216
00:10:28,233 --> 00:10:31,733
中心

217
00:10:31,766 --> 00:10:45,633
[音乐]

218
00:10:45,633 --> 00:10:47,900
最后我们看到它开始

219
00:10:47,900 --> 00:10:50,800
在同伴健身房中挑战布洛克 战斗

220
00:10:50,833 --> 00:10:52,366
比其他战斗要困难得多，并且

221
00:10:52,400 --> 00:10:54,300
构成了重大挑战，因为人工智能

222
00:10:54,300 --> 00:10:56,166
以前的经验现在正在

223
00:10:56,200 --> 00:10:58,866
对抗它，到目前为止，它

224
00:10:58,866 --> 00:11:00,600
仅使用主要动作就取得了巨大的成功，

225
00:11:00,633 --> 00:11:02,733
并且已经学会了

226
00:11:02,733 --> 00:11:05,433
完全依赖它们，现在它需要使用

227
00:11:05,466 --> 00:11:07,766
其他的东西这个问题可能看起来

228
00:11:07,800 --> 00:11:09,866
微不足道，但即使人类也在

229
00:11:09,866 --> 00:11:12,200
同样的基本问题上挣扎，我们的

230
00:11:12,233 --> 00:11:14,533
经验和偏见帮助我们

231
00:11:14,533 --> 00:11:16,333
更快地做出决策和解决问题，

232
00:11:16,333 --> 00:11:18,633
但它们也限制了我们的思维

233
00:11:18,666 --> 00:11:20,433
并阻碍我们

234
00:11:20,466 --> 00:11:23,266
从新角度处理问题的能力

235
00:11:23,266 --> 00:11:25,333
更糟糕的是，如果人工智能输掉战斗

236
00:11:25,333 --> 00:11:27,366
太多次，它实际上会学会一起

237
00:11:27,400 --> 00:11:30,233
避免它，但在

238
00:11:30,233 --> 00:11:32,000
仔细调整我们的奖励和

239
00:11:32,033 --> 00:11:36,166
无数次失败的尝试之后，人工智能终于在我们

240
00:11:36,200 --> 00:11:38,933
看到它开始健身房战斗的一场游戏中获得了一丝运气。

241
00:11:38,933 --> 00:11:41,400
只有一个

242
00:11:41,433 --> 00:11:43,933
可用的神奇宝贝，

243
00:11:43,933 --> 00:11:46,233
其可用动作中的生命值只有一小部分，

244
00:11:46,266 --> 00:11:48,400
通常这将是一个非常糟糕的

245
00:11:48,433 --> 00:11:50,800
策略，它不知道水

246
00:11:50,833 --> 00:11:52,533
型动作

247
00:11:52,533 --> 00:11:53,800
对岩石神奇

248
00:11:53,833 --> 00:11:56,300
宝贝非常有效，但因为铲球完全

249
00:11:56,300 --> 00:11:58,300
耗尽了它 看到它需要使用

250
00:11:58,300 --> 00:12:03,833
替代动作并切换到泡泡

251
00:12:03,866 --> 00:12:07,233
，最终经过 300 多天

252
00:12:07,266 --> 00:12:09,566
的模拟游戏时间和 100 次

253
00:12:09,566 --> 00:12:12,033
学习迭代，AI

254
00:12:12,066 --> 00:12:14,500
第一次击败了布洛克，

255
00:12:14,500 --> 00:12:17,233
这一意外的突破帮助

256
00:12:17,266 --> 00:12:19,600
它学会选择泡泡作为其默认

257
00:12:19,633 --> 00:12:21,966
动作，并且 在未来的游戏中，它会

258
00:12:22,000 --> 00:12:23,533
更加

259
00:12:23,533 --> 00:12:27,933
一致地赢得这场战斗，这确实超出了我

260
00:12:27,933 --> 00:12:30,233
在开始这个项目时所认为可能实现的预期，确实需要

261
00:12:30,266 --> 00:12:32,333
进行大量的实验才能

262
00:12:32,333 --> 00:12:34,600
到达这里，但每次我检查训练时我仍然感到惊讶

263
00:12:34,633 --> 00:12:36,566
运行并

264
00:12:36,566 --> 00:12:38,500
发现人工智能已经到达了一个新的

265
00:12:38,533 --> 00:12:41,166
领域，我什至可以说这是一次

266
00:12:41,200 --> 00:12:43,266
非常有益的

267
00:12:43,266 --> 00:12:45,133
体验，所以这似乎是一个

268
00:12:45,133 --> 00:12:46,833
合理的停止点，但

269
00:12:46,866 --> 00:12:49,200
出于好奇，让我们看看

270
00:12:49,233 --> 00:12:59,633
如果我们放任它，人工智能会走多远

271
00:12:59,633 --> 00:13:01,366
体育馆战斗结束后继续，它开始

272
00:13:01,366 --> 00:13:27,400
在[音乐]三号路线上遇到训练家，

273
00:13:27,433 --> 00:13:31,333
最终它到达

274
00:13:31,333 --> 00:13:34,466
神奇宝贝中心内的月亮山入口，这里一个

275
00:13:34,500 --> 00:13:36,733
人会以 500 美元的价格卖给你一个鲤鱼王，

276
00:13:36,733 --> 00:13:39,333
鲤鱼王在短期内根本没有帮助，

277
00:13:39,333 --> 00:13:41,000
所以你 可能会认为

278
00:13:41,033 --> 00:13:43,900
AI 不会对此感兴趣，但是

279
00:13:43,900 --> 00:13:45,900
购买它是获得

280
00:13:45,900 --> 00:13:49,333
5 个等级的超级简单方法，因此 AI 每次都会

281
00:13:49,333 --> 00:13:51,900
在所有游戏中购买它，总共购买

282
00:13:51,933 --> 00:13:55,166
超过 10,000 个魔鲤，AI 的行为

283
00:13:55,200 --> 00:13:57,200
可能看起来很愚蠢 当它的目标

284
00:13:57,200 --> 00:13:59,033
偏离其预期

285
00:13:59,033 --> 00:14:00,766
目的时，但有时

286
00:14:00,800 --> 00:14:03,033
同样的事情也会发生在人类身上，为了

287
00:14:03,033 --> 00:14:04,966
让人工智能在游戏中取得进步，我们将

288
00:14:05,000 --> 00:14:06,900
其配置为寻找提高

289
00:14:06,900 --> 00:14:09,966
其水平的方法，类似地，进化

290
00:14:10,000 --> 00:14:16,900
选择了人类作为猎人生存

291
00:14:16,900 --> 00:14:18,900
当人工智能到达一个有廉价

292
00:14:18,900 --> 00:14:20,800
但毫无意义的鲤鱼王的地区时，它会自然而然地让我们找到获取稀缺食物的方法，它

293
00:14:20,833 --> 00:14:22,466
每次都会购买神奇宝贝，因为这会

294
00:14:22,466 --> 00:14:25,100
增加其总水平，并且随着人类

295
00:14:25,100 --> 00:14:27,033
已经达到了物质丰富的现代时代，

296
00:14:27,066 --> 00:14:29,133
我们本能地购买 不健康的食物，

297
00:14:29,133 --> 00:14:30,600
因为它们含有历史上

298
00:14:30,633 --> 00:14:33,366
稀缺的营养素，每个代理

299
00:14:33,366 --> 00:14:35,166
目标都产生于非常不同的

300
00:14:35,200 --> 00:14:36,533
情况，但当它们的

301
00:14:36,533 --> 00:14:38,333
环境发生变化时，它们都变得

302
00:14:38,333 --> 00:14:40,266
错位，不再支持它们

303
00:14:40,300 --> 00:14:42,733
最初的

304
00:14:42,733 --> 00:14:47,066
目标，接下来人工智能开始探索

305
00:14:47,100 --> 00:14:57,866
[音乐]

306
00:14:57,866 --> 00:15:00,966
月亮山内的洞穴

307
00:15:01,000 --> 00:15:19,533
[ 音乐]

308
00:15:19,533 --> 00:15:21,733
到目前为止，它已经与遇到的所有野生神奇

309
00:15:21,733 --> 00:15:23,833
宝贝进行了战斗，但

310
00:15:23,866 --> 00:15:26,100
鲤鱼王的效率太低了，以至于它

311
00:15:26,100 --> 00:15:28,100
最终学会了一种专门针对

312
00:15:28,100 --> 00:15:30,833
这只神奇宝贝的特殊行为，如果鲤鱼王

313
00:15:30,866 --> 00:15:33,100
在任何情况下被派去战斗，它

314
00:15:33,100 --> 00:15:35,333
会尝试逃跑，无论什么情况，

315
00:15:35,333 --> 00:15:38,766
有时甚至

316
00:15:38,800 --> 00:15:41,233
尽管探索了很多洞穴，但与[音乐]训练师战斗时，

317
00:15:41,233 --> 00:15:43,300
它似乎卡在了这段

318
00:15:43,300 --> 00:15:45,633
通道中，可能是因为该区域在

319
00:15:45,666 --> 00:15:47,166
视觉上太统一，无法触发

320
00:15:47,200 --> 00:15:49,933
探索奖励，如果我们将

321
00:15:49,933 --> 00:15:51,833
游戏运行时间延长到超过 2 小时，它就

322
00:15:51,866 --> 00:15:54,500
能够进化 Blastoise 和 Pidgeot，但

323
00:15:54,500 --> 00:15:55,800
它永远不会一路通过

324
00:15:55,833 --> 00:15:57,966
Mount Moon，我们可以继续尝试

325
00:15:58,000 --> 00:16:00,433
改进我们的奖励功能，但相反，

326
00:16:00,433 --> 00:16:02,000
我们将以此为停止

327
00:16:02,033 --> 00:16:04,700
点，并开始分析 AI

328
00:16:04,700 --> 00:16:16,100
[音乐] 从

329
00:16:16,100 --> 00:16:18,800
该可视化中学到了什么 显示人工智能如何

330
00:16:18,833 --> 00:16:22,066
在地图上导航 每个箭头

331
00:16:22,066 --> 00:16:23,900
表示它

332
00:16:23,900 --> 00:16:30,700
站在特定地点时移动的平均方向

333
00:16:30,700 --> 00:16:43,766
[音乐]

334
00:16:43,800 --> 00:16:46,100
一个有趣的模式是，它似乎更

335
00:16:46,100 --> 00:16:48,600
喜欢在

336
00:16:48,633 --> 00:16:51,566
地图的几乎所有边缘上逆时针行走，这意味着

337
00:16:51,566 --> 00:16:53,066
当站立时 当边缘在

338
00:16:53,100 --> 00:16:55,300
右侧时，它更喜欢向上行走，

339
00:16:55,300 --> 00:16:57,466
显示为蓝色；当其上方有边缘时，

340
00:16:57,500 --> 00:17:02,566
它更喜欢向左行走；当边缘在左侧时，它更

341
00:17:02,566 --> 00:17:05,166
喜欢向下行走，显示为橙色；当边缘在左边时，它更喜欢向下行走，

342
00:17:05,200 --> 00:17:06,966
当有边缘时，它更喜欢向下行走 在它下面的边缘，它更喜欢

343
00:17:06,966 --> 00:17:10,066
向右行走（以

344
00:17:10,100 --> 00:17:12,433
绿色显示），很难确定它为什么会

345
00:17:12,466 --> 00:17:15,233
出现这种行为，因为解释

346
00:17:15,266 --> 00:17:17,133
是，如果你在二维空间的周边行走，这个优势可以帮助它

347
00:17:17,133 --> 00:17:20,966
以有限的记忆和规划进行导航。

348
00:17:20,966 --> 00:17:22,666
我们保证

349
00:17:22,700 --> 00:17:25,400
通过所有入口和出口点，因此通过

350
00:17:25,433 --> 00:17:26,866
选择一个方向并沿着一条

351
00:17:26,866 --> 00:17:28,933
边缘，您可以到达所有可能的

352
00:17:28,933 --> 00:17:31,233
交叉点人工智能并不总是遵循

353
00:17:31,266 --> 00:17:32,933
这种模式，但它似乎确实可以帮助它

354
00:17:32,933 --> 00:17:36,000
在偏离轨道后恢复，但是

355
00:17:36,033 --> 00:17:37,900
有一些位置 导航

356
00:17:37,900 --> 00:17:39,800
失败并且人工智能被物理

357
00:17:39,833 --> 00:17:42,500
卡住的地方，例如在 22 号公路的底部，

358
00:17:42,500 --> 00:17:45,033
有一个很长的区域，没有任何

359
00:17:45,066 --> 00:17:47,533
用处，顶部的单向壁架

360
00:17:47,533 --> 00:17:49,200
意味着有很多方法可以进入

361
00:17:49,233 --> 00:17:51,033
该区域，但只有一个位置可以进入

362
00:17:51,033 --> 00:17:53,433
该区域 可以让它像

363
00:17:53,433 --> 00:17:55,900
捕蝇器一样工作，人工智能的随机运动会

364
00:17:55,900 --> 00:17:57,700
导致它被卡住并花费

365
00:17:57,700 --> 00:17:59,133
不成比例的时间，

366
00:17:59,133 --> 00:18:01,800
我们还可以想象人工智能

367
00:18:01,833 --> 00:18:03,933
在训练过程中的行为如何变化，

368
00:18:03,933 --> 00:18:05,800
这里显示了早期的训练

369
00:18:05,833 --> 00:18:07,966
迭代 蓝色表示中间的

370
00:18:07,966 --> 00:18:10,033
训练迭代以绿色显示

371
00:18:10,066 --> 00:18:42,766
，后面的迭代以红色显示，

372
00:18:42,800 --> 00:18:44,466
我们可以清楚地看到，在

373
00:18:44,466 --> 00:18:46,300
训练过程中，AI 采取了

374
00:18:46,300 --> 00:18:48,233
一条穿过 vidiian 森林的路径，但

375
00:18:48,266 --> 00:18:49,866
后来它切换到了另一条

376
00:18:49,866 --> 00:18:51,833
[音乐]

377
00:18:51,866 --> 00:18:56,200
路径 在训练过程中形成的另一个有趣的行为

378
00:18:56,233 --> 00:18:57,900
发生在游戏的最初几分钟，

379
00:18:57,900 --> 00:19:00,333
由于某种原因，它

380
00:19:00,333 --> 00:19:02,466
以完全相同的按钮按下顺序开始每场游戏，

381
00:19:02,500 --> 00:19:05,233
这很令人困惑，

382
00:19:05,266 --> 00:19:07,133
特别是因为运动甚至没有

383
00:19:07,133 --> 00:19:10,133
遵循最佳路径观看

384
00:19:10,133 --> 00:19:11,666
再进一步，但是

385
00:19:11,700 --> 00:19:14,566
发生了一些有趣的事情，它在

386
00:19:14,600 --> 00:19:16,533
第一次遇到时立即抛出一个 Pokeball，并

387
00:19:16,533 --> 00:19:18,966
在第一次尝试时成功，这让

388
00:19:18,966 --> 00:19:20,900
我意识到，即使游戏

389
00:19:20,933 --> 00:19:23,266
包含许多随机元素，它

390
00:19:23,300 --> 00:19:25,366
仍然会

391
00:19:25,400 --> 00:19:26,866
根据玩家的

392
00:19:26,866 --> 00:19:29,133
输入确定性地表现出来 对于

393
00:19:29,133 --> 00:19:31,233
速通玩家来说众所周知，人工智能似乎

394
00:19:31,266 --> 00:19:33,066
利用了这样一个事实：它

395
00:19:33,066 --> 00:19:37,600
在每场比赛中都从相同的状态开始，在

396
00:19:37,633 --> 00:19:40,466
第一次尝试时就可靠地捕捉到神奇宝贝，我们还可以可视化其他

397
00:19:40,466 --> 00:19:42,333
统计数据，以了解

398
00:19:42,333 --> 00:19:44,533
所有比赛中发生的情况 左边是人工智能玩的游戏，

399
00:19:44,533 --> 00:19:46,533
我们可以在右边看到

400
00:19:46,533 --> 00:19:49,200
人工智能至少捕获过一次的每个神奇宝贝，

401
00:19:49,233 --> 00:19:50,633
右边我们可以看到在训练过程中的哪个点

402
00:19:50,633 --> 00:19:52,900
捕获这些神奇宝贝，

403
00:19:52,900 --> 00:19:54,600
每个区域的高度都是对

404
00:19:54,633 --> 00:19:56,500
数缩放的，因此

405
00:19:56,500 --> 00:19:58,066
代表较厚的神奇宝贝 区域被

406
00:19:58,100 --> 00:20:00,333
捕获了数千次，而

407
00:20:00,333 --> 00:20:02,266
代表较薄区域的口袋妖怪

408
00:20:02,300 --> 00:20:07,566
可能只被捕获了

409
00:20:07,600 --> 00:20:10,100
几次，反思这一切，

410
00:20:10,100 --> 00:20:12,000
令人难以置信的是，

411
00:20:12,033 --> 00:20:13,700
玩

412
00:20:13,700 --> 00:20:16,133
视频游戏的算法可以产生相关的体验，在这数万次的过程中发生了如此多的事情。 几个

413
00:20:16,133 --> 00:20:18,133
小时，

414
00:20:18,133 --> 00:20:20,066
根本没有足够的时间来发现所有

415
00:20:20,100 --> 00:20:21,900
有趣的故事，更不用说

416
00:20:21,900 --> 00:20:24,566
记录它们了，因此我们

417
00:20:24,600 --> 00:20:26,700
在下一节中总结视频的主要部分，

418
00:20:26,700 --> 00:20:28,433
我们将深入研究一些技术

419
00:20:28,466 --> 00:20:30,566
细节，探索

420
00:20:30,600 --> 00:20:32,800
有效运行实验的策略，考虑未来的

421
00:20:32,800 --> 00:20:34,600
改进和 回顾一下如何自己运行这个

422
00:20:34,600 --> 00:20:37,066
程序，尽管到目前为止我已经尽力

423
00:20:37,066 --> 00:20:39,266
避免它，这

424
00:20:39,266 --> 00:20:40,966
部分将不可避免地包含更多的

425
00:20:40,966 --> 00:20:42,466
技术

426
00:20:42,500 --> 00:20:44,633
术语，首先用于训练这个人工智能的特定

427
00:20:44,666 --> 00:20:46,266
强化学习算法

428
00:20:46,266 --> 00:20:48,666
被称为近端策略

429
00:20:48,666 --> 00:20:50,700
优化，它是一个 相当标准的

430
00:20:50,700 --> 00:20:52,733
现代强化学习算法

431
00:20:52,733 --> 00:20:53,933
，尽管它最初是

432
00:20:53,933 --> 00:20:55,600
在游戏和机器人技术的背景下设计的，但它

433
00:20:55,633 --> 00:20:57,500
也被用作

434
00:20:57,500 --> 00:20:59,233
创建有用的大型

435
00:20:59,266 --> 00:21:01,500
语言模型的最后一步，然而，虽然

436
00:21:01,500 --> 00:21:03,166
强化学习有时

437
00:21:03,200 --> 00:21:05,233
感觉像魔法，但实际上可能

438
00:21:05,233 --> 00:21:09,700
非常困难 在实践中应用的工具

439
00:21:09,700 --> 00:21:11,566
机器学习的基本挑战是让程序在

440
00:21:11,600 --> 00:21:13,633
没有明确告诉

441
00:21:13,633 --> 00:21:16,033
它如何做的情况下做某事，这意味着如果您的

442
00:21:16,033 --> 00:21:17,433
模型没有按照您预期的方式运行，

443
00:21:17,433 --> 00:21:19,533
您必须弄清楚如何间接

444
00:21:19,533 --> 00:21:21,833
改进它 就其

445
00:21:21,866 --> 00:21:24,300
学习算法或训练数据而言，

446
00:21:24,300 --> 00:21:26,433
在线强化学习

447
00:21:26,466 --> 00:21:28,733
在此基础上添加了额外的间接层，

448
00:21:28,733 --> 00:21:30,966
输入到模型中的训练数据

449
00:21:30,966 --> 00:21:33,100
不再是固定的并

450
00:21:33,133 --> 00:21:35,866
在您的控制之下，而是本身是

451
00:21:35,900 --> 00:21:38,000
模型早期行为的产物

452
00:21:38,033 --> 00:21:42,533
这个反馈循环会导致

453
00:21:42,533 --> 00:21:44,966
无法预测的突发行为，因此这里有一些

454
00:21:44,966 --> 00:21:47,033
策略可以在

455
00:21:47,066 --> 00:21:50,100
没有机构规模资源的情况下应对这些挑战，

456
00:21:50,100 --> 00:21:52,100
首先可能有必要简化

457
00:21:52,100 --> 00:21:53,533
您的问题，以解决

458
00:21:53,533 --> 00:21:55,866
您可能已经注意到的工具的局限性

459
00:21:55,900 --> 00:21:57,833
在视频的前面，人工智能

460
00:21:57,866 --> 00:21:59,500
实际上并不是从

461
00:21:59,500 --> 00:22:02,033
游戏一开始就开始的，在这里你可以看到

462
00:22:02,066 --> 00:22:03,900
人工智能的早期版本，它确实

463
00:22:03,900 --> 00:22:08,100
从

464
00:22:08,133 --> 00:22:10,533
[音乐]开始，

465
00:22:10,533 --> 00:22:12,766
它在选择神奇宝贝时没有任何问题

466
00:22:12,800 --> 00:22:15,100
， 赢得第一

467
00:22:15,100 --> 00:22:17,433
场战斗后，

468
00:22:17,466 --> 00:22:19,200
当需要从

469
00:22:19,233 --> 00:22:21,533
维迪安市原路返回托盘镇时，问题就会出现，这是

470
00:22:21,533 --> 00:22:23,466
因为探索奖励并没有

471
00:22:23,500 --> 00:22:25,433
给它任何动力返回到

472
00:22:25,466 --> 00:22:27,800
已经访问过的区域，也许

473
00:22:27,833 --> 00:22:29,700
可以解决这个问题 通过针对

474
00:22:29,700 --> 00:22:32,033
这种情况设置特殊奖励，

475
00:22:32,066 --> 00:22:34,000
但我认为简单地跳过

476
00:22:34,033 --> 00:22:35,933
这一步是更好地利用时间，

477
00:22:35,933 --> 00:22:37,933
修改后的起点仍然非常

478
00:22:37,933 --> 00:22:39,766
接近原始点，并且

479
00:22:39,800 --> 00:22:41,666
默认情况下给予人工智能杰尼龟，它以后有更好的

480
00:22:41,666 --> 00:22:44,600
成功机会 接下来

481
00:22:44,633 --> 00:22:46,133
重要的是找到一种设置，允许

482
00:22:46,133 --> 00:22:48,133
您在

483
00:22:48,133 --> 00:22:50,533
合理的时间和成本内迭代实验，在

484
00:22:50,533 --> 00:22:52,533
许多情况下，瓶颈将是

485
00:22:52,533 --> 00:22:54,633
模拟或操作

486
00:22:54,666 --> 00:22:57,366
人工智能与之交互的环境，在我们的

487
00:22:57,400 --> 00:22:59,333
例子中，环境是 Pokémon Red

488
00:22:59,333 --> 00:23:01,933
运行 在 Game Boy 模拟器上 P boy

489
00:23:01,933 --> 00:23:04,066
这个模拟器

490
00:23:04,100 --> 00:23:09,166
在单个现代 CPU 核心上以正常速度 20 倍的速度运行，在具有

491
00:23:09,200 --> 00:23:11,500
多个核心的大型服务器上并行运行迷你游戏，使

492
00:23:11,500 --> 00:23:13,433
我们能够

493
00:23:13,466 --> 00:23:15,700
以超过 1,000 倍的速度有效地收集与环境的交互

494
00:23:15,700 --> 00:23:22,700
比正常速度快，这意味着

495
00:23:22,700 --> 00:23:25,300
如果我们使用小模型作为策略，每次持续 2 小时的一批 40 个游戏的学习迭代将在大约 6 分钟内完成，

496
00:23:25,300 --> 00:23:27,300
其推理和

497
00:23:27,300 --> 00:23:29,033
训练时间将可以忽略不计，

498
00:23:29,066 --> 00:23:31,700
特别是如果我们使用 GPU，这意味着我们

499
00:23:31,700 --> 00:23:33,600
可以

500
00:23:33,633 --> 00:23:35,733
在几分钟到几小时内获得小型实验的结果，完整的训练

501
00:23:35,733 --> 00:23:38,200
运行将需要几天的时间，

502
00:23:38,233 --> 00:23:40,700
使用最

503
00:23:40,700 --> 00:23:42,566
便宜的云选项，这很容易变得相当昂贵，单个完整的

504
00:23:42,600 --> 00:23:44,333
训练运行成本约为

505
00:23:44,333 --> 00:23:46,966
50 美元，该项目中运行的所有实验的

506
00:23:46,966 --> 00:23:49,966
综合成本 总共大约

507
00:23:49,966 --> 00:23:52,166
1,000 美元，但是如果您不小心

508
00:23:52,200 --> 00:23:54,100
选择和管理这些资源，很

509
00:23:54,100 --> 00:23:57,366
容易花费更多的钱，接下来

510
00:23:57,400 --> 00:23:58,833
您需要仔细考虑

511
00:23:58,833 --> 00:24:00,166
人工智能如何与

512
00:24:00,200 --> 00:24:01,966
环境交互以及您的奖励函数

513
00:24:01,966 --> 00:24:04,433
是如何设计的 我在这个项目中所做的选择

514
00:24:04,466 --> 00:24:06,566
当然并不都是最佳的，

515
00:24:06,600 --> 00:24:07,966
但我至少会描述

516
00:24:07,966 --> 00:24:10,600
其中的一些考虑因素，

517
00:24:10,633 --> 00:24:12,900
例如 AI 观察屏幕并

518
00:24:12,900 --> 00:24:16,300
每 24 帧选择一次动作，

519
00:24:16,300 --> 00:24:18,133
这足以让玩家

520
00:24:18,133 --> 00:24:20,233
移动世界中的一个网格空间，因此

521
00:24:20,266 --> 00:24:22,366
每当人工智能观察屏幕时，它

522
00:24:22,400 --> 00:24:24,100
总是会在其中一个网格单元上完美对齐，

523
00:24:24,100 --> 00:24:26,200
这使得

524
00:24:26,233 --> 00:24:28,533
探索奖励更加有效，

525
00:24:28,533 --> 00:24:30,166
因为它显着限制了

526
00:24:30,200 --> 00:24:31,900
可以看到的组件的可能屏幕数量。

527
00:24:31,900 --> 00:24:34,533
做出

528
00:24:34,533 --> 00:24:36,633
称为策略的决策由一个

529
00:24:36,666 --> 00:24:39,000
小型卷积神经网络表示，

530
00:24:39,033 --> 00:24:41,200
值得注意的是它是非循环的，这意味着

531
00:24:41,233 --> 00:24:43,033
它实际上没有过去的内部记忆，

532
00:24:43,033 --> 00:24:45,666
这样做是为了提高

533
00:24:45,666 --> 00:24:48,200
训练稳定性收敛速度和

534
00:24:48,233 --> 00:24:50,333
简单性那么它如何在

535
00:24:50,333 --> 00:24:53,066
没有记忆的情况下做出决策 首先，

536
00:24:53,100 --> 00:24:54,800
三个最近的屏幕

537
00:24:54,833 --> 00:24:56,700
堆叠在一起，创建一种简单的

538
00:24:56,700 --> 00:24:59,200
短期记忆形式；其次，

539
00:24:59,233 --> 00:25:00,900
有关游戏状态的一些基本信息被

540
00:25:00,900 --> 00:25:03,500
编码为视觉状态栏，这些

541
00:25:03,500 --> 00:25:05,566
显示生命值、总级别和

542
00:25:05,566 --> 00:25:08,200
探索进度，这是一种更传统的

543
00:25:08,233 --> 00:25:09,933
方法 是将这些

544
00:25:09,933 --> 00:25:12,033
编码为直接注入到模型中的抽象向量，

545
00:25:12,066 --> 00:25:14,500
我选择了这种方法，

546
00:25:14,500 --> 00:25:16,000
因为它是人类和机器

547
00:25:16,033 --> 00:25:17,866
可解释的，这使得在一起

548
00:25:17,866 --> 00:25:19,466
调试录制的游戏时更容易知道发生了什么，

549
00:25:19,466 --> 00:25:22,400
这些编码了

550
00:25:22,433 --> 00:25:24,100
足够的信息，使模型可以在

551
00:25:24,100 --> 00:25:26,366
没有任何信息的情况下做出决策 任何其他形式的

552
00:25:26,400 --> 00:25:28,433
记忆现在让我们谈谈

553
00:25:28,433 --> 00:25:30,866
奖励函数探索和

554
00:25:30,866 --> 00:25:32,600
总水平在视频前面讨论过

555
00:25:32,633 --> 00:25:34,933
这些是迄今为止最

556
00:25:34,933 --> 00:25:36,866
重要的奖励，但实际上总共

557
00:25:36,900 --> 00:25:39,066
使用了七个，甚至更多

558
00:25:39,100 --> 00:25:41,400
经过了测试但不是最终的

559
00:25:41,433 --> 00:25:42,933
没有足够的时间来深入介绍

560
00:25:42,933 --> 00:25:44,933
所有这些内容，但

561
00:25:44,933 --> 00:25:46,600
所有这些内容的一般标准是，它们广泛

562
00:25:46,633 --> 00:25:48,533
鼓励玩好游戏，而

563
00:25:48,533 --> 00:25:50,466
不是专注于特定时刻，并且

564
00:25:50,500 --> 00:25:52,166
它们不应该轻易

565
00:25:52,200 --> 00:25:54,466
欺骗所有这些信息

566
00:25:54,466 --> 00:25:59,000
游戏状态是通过从 Game Boy 模拟器内存​​中读取值来获取的。

567
00:25:59,033 --> 00:26:01,366
游戏没有任何类型的适当 API，

568
00:26:01,400 --> 00:26:03,066
但其大部分内存是静态

569
00:26:03,066 --> 00:26:04,933
分配的，因此变量始终可以

570
00:26:04,933 --> 00:26:09,400
在 Pet 项目已完成的相同内存地址中找到 对

571
00:26:09,433 --> 00:26:11,100
游戏源代码进行逆向工程

572
00:26:11,100 --> 00:26:13,933
并将其内存映射为

573
00:26:13,933 --> 00:26:18,466
切线的工作非常出色，老实说，

574
00:26:18,500 --> 00:26:20,600
整个游戏的所有逻辑图形和音频都存储在不到 1

575
00:26:20,633 --> 00:26:23,166
MB 的空间中，这比您看到的一张照片还小，这真是令人兴奋

576
00:26:23,200 --> 00:26:26,900
无论如何，在你的手机上

577
00:26:26,900 --> 00:26:28,400
理解人工智能的行为

578
00:26:28,433 --> 00:26:29,933
对于实现其全部

579
00:26:29,933 --> 00:26:32,133
潜力至关重要，最好的方法之一

580
00:26:32,133 --> 00:26:33,466
是通过

581
00:26:33,500 --> 00:26:35,133
可视化，所以

582
00:26:35,133 --> 00:26:37,933
这个项目中使用的可视化是如何制作的

583
00:26:37,933 --> 00:26:39,866
第一个关键信息，如玩家

584
00:26:39,900 --> 00:26:41,766
坐标和神奇宝贝统计数据

585
00:26:41,800 --> 00:26:44,433
记录在 每个游戏中的每一步

586
00:26:44,433 --> 00:26:45,900
接下来所有游戏都需要

587
00:26:45,900 --> 00:26:48,300
渲染到单个地图上 游戏

588
00:26:48,300 --> 00:26:50,333
本身不包含具有

589
00:26:50,333 --> 00:26:52,133
全局坐标系的单个地图的概念，

590
00:26:52,133 --> 00:26:54,666
而是将世界分为

591
00:26:54,700 --> 00:26:58,600
游戏跟踪的 256x 256 块

592
00:26:58,633 --> 00:27:00,433
玩家在块内的本地坐标

593
00:27:00,433 --> 00:27:02,100
以及玩家

594
00:27:02,100 --> 00:27:04,600
当前所在的块，因此在导出

595
00:27:04,633 --> 00:27:06,333
从本地块坐标到全局

596
00:27:06,333 --> 00:27:08,600
坐标的映射后，所有游戏都可以

597
00:27:08,633 --> 00:27:11,033
一起渲染，实际渲染本身是

598
00:27:11,033 --> 00:27:13,166
一个相当慢的 numpy 程序，它将

599
00:27:13,200 --> 00:27:14,733
精灵放置在坐标处 玩家

600
00:27:14,733 --> 00:27:16,466
正在移动，

601
00:27:16,500 --> 00:27:17,966
根据他们移动的方向选择合适的精灵，

602
00:27:17,966 --> 00:27:19,866
并在每个图块之间进行插值

603
00:27:19,900 --> 00:27:22,533
以渲染巨人游戏网格，

604
00:27:22,533 --> 00:27:24,600
使用 python 脚本生成 FFM

605
00:27:24,633 --> 00:27:26,433
Peg 命令，

606
00:27:26,433 --> 00:27:29,633
使用功能强大的服务器将数千个视频拼贴在一起

607
00:27:29,633 --> 00:27:31,633
可视化是使用

608
00:27:31,633 --> 00:27:33,666
相同的玩家数据通过聚合

609
00:27:33,666 --> 00:27:35,333
每个图块上的所有动作来实现的，

610
00:27:35,333 --> 00:27:38,166
结合所有这些策略，

611
00:27:38,200 --> 00:27:39,566
可以训练强化

612
00:27:39,566 --> 00:27:41,500
学习代理

613
00:27:41,533 --> 00:27:43,866
仅使用适度的资源来玩复杂的游戏，但是还

614
00:27:43,900 --> 00:27:45,433
可以做些什么来进一步改进

615
00:27:45,466 --> 00:27:47,600
这个过程让我们考虑一下 首先

616
00:27:47,633 --> 00:27:49,366
考虑如何在未来变得更容易、

617
00:27:49,400 --> 00:27:51,700
更便宜、更快，

618
00:27:51,700 --> 00:27:54,766
正如前面提到的，

619
00:27:54,800 --> 00:27:56,400
这个项目中的人工智能从头开始学习，

620
00:27:56,400 --> 00:27:58,266
没有任何先前的 EXP 经验，

621
00:27:58,266 --> 00:28:00,666
将来可能会

622
00:28:00,666 --> 00:28:02,066
应用一种称为

623
00:28:02,066 --> 00:28:04,300
迁移学习的东西，这就是 当一个模型

624
00:28:04,300 --> 00:28:06,666
在大量广泛的数据集上进行预训练时，该模型

625
00:28:06,700 --> 00:28:08,200
可以

626
00:28:08,233 --> 00:28:11,033
非常有效地用于过去的新任务，这

627
00:28:11,033 --> 00:28:12,500
已经彻底改变了

628
00:28:12,500 --> 00:28:14,133
计算机视觉和自然语言

629
00:28:14,133 --> 00:28:16,066
处理领域，有一些有趣的

630
00:28:16,100 --> 00:28:18,233
早期工作将其应用于强化学习，但它 还

631
00:28:18,266 --> 00:28:20,600
没有真正落地，这部分是

632
00:28:20,633 --> 00:28:22,766
由于这些类型的任务缺乏大量不同的数据集，

633
00:28:22,800 --> 00:28:26,733
但是

634
00:28:26,733 --> 00:28:28,533
从足够大的数据集中提取有用的世界模型似乎应该是可行的，

635
00:28:28,533 --> 00:28:30,433
这是

636
00:28:30,466 --> 00:28:32,533
我使用的一个快速实验

637
00:28:32,533 --> 00:28:34,933
游戏状态零镜头分类的剪辑很

638
00:28:34,933 --> 00:28:36,433
容易想象如何使用它

639
00:28:36,466 --> 00:28:38,066
为新环境创建奖励函数，而

640
00:28:38,066 --> 00:28:39,966
无需特殊访问

641
00:28:39,966 --> 00:28:42,166
其内部状态，

642
00:28:42,200 --> 00:28:44,533
大型多模式

643
00:28:44,533 --> 00:28:47,200
模型开始对此产生巨大影响可能只是时间问题

644
00:28:47,233 --> 00:28:49,566
第二种感兴趣的方法是

645
00:28:49,566 --> 00:28:51,666
直接学习环境模型，

646
00:28:51,700 --> 00:28:53,366
该领域的一些著名作品是

647
00:28:53,400 --> 00:28:56,100
muzo 和 dreamer，这些方法

648
00:28:56,100 --> 00:28:58,333
通过

649
00:28:58,333 --> 00:28:59,800
学习环境本身的模型

650
00:28:59,833 --> 00:29:02,066
并根据

651
00:29:02,066 --> 00:29:04,333
我最近运行的学习模型优化策略，极大地提高了数据效率

652
00:29:04,333 --> 00:29:08,000
用 Dreamer V3 进行了一些实验，

653
00:29:08,033 --> 00:29:10,500
结果给我留下了深刻的印象第三种值得一提的方法

654
00:29:10,500 --> 00:29:12,500
是分层强化学习，它将

655
00:29:12,500 --> 00:29:14,233
低级控制与高级

656
00:29:14,266 --> 00:29:16,266
规划分离，这允许细粒度

657
00:29:16,266 --> 00:29:18,200
运动和长期策略

658
00:29:18,233 --> 00:29:19,533
由单独的

659
00:29:19,533 --> 00:29:21,566
机制处理，所以这些是一些

660
00:29:21,566 --> 00:29:22,900
未来可以改进这项技术的方式

661
00:29:22,933 --> 00:29:25,366
总结起来，让我们逐步了解如何

662
00:29:25,400 --> 00:29:28,100
在您自己的计算机上运行此人工智能，

663
00:29:28,100 --> 00:29:30,933
这是视频描述中链接的存储库，

664
00:29:30,933 --> 00:29:32,833
第一步是

665
00:29:32,866 --> 00:29:36,066
下载它，您可以使用以下命令来执行此操作

666
00:29:36,066 --> 00:29:38,500
git 或下载

667
00:29:38,500 --> 00:29:40,633
zip 下一步将是

668
00:29:40,666 --> 00:29:43,600
合法获取你的 Pokemon Red Rom，你

669
00:29:43,633 --> 00:29:45,333
可以使用 Google 找到它，它应该是

670
00:29:45,333 --> 00:29:47,433
一个以 ingb 结尾的 1 MB 文件，

671
00:29:47,466 --> 00:29:50,133
我已经在这里了，所以我们

672
00:29:50,133 --> 00:29:52,800
将进入

673
00:29:52,833 --> 00:29:55,633
存储库的根目录，我们

674
00:29:55,633 --> 00:29:59,633
将其复制到

675
00:29:59,633 --> 00:30:04,666
此处，并将其重命名为

676
00:30:04,666 --> 00:30:09,100
Pokemon red。  GB 现在，下一步

677
00:30:09,100 --> 00:30:11,200
是

678
00:30:11,233 --> 00:30:15,366
可选的，嗯，我们可以创建一个 cond

679
00:30:15,400 --> 00:30:19,233
环境，这样我们就可以

680
00:30:19,233 --> 00:30:28,100
将其命名为您想要的任何名称，

681
00:30:28,100 --> 00:30:31,100
接受

682
00:30:31,100 --> 00:30:35,566
它，然后激活

683
00:30:35,566 --> 00:30:37,000
[音乐]

684
00:30:37,033 --> 00:30:39,466
它，现在下一步是相同的，

685
00:30:39,466 --> 00:30:41,266
无论您创建的是 cond 环境

686
00:30:41,266 --> 00:30:44,200
还是 不是我们只是要安装

687
00:30:44,233 --> 00:30:46,233
需求文件中的需求，

688
00:30:46,233 --> 00:30:48,833
例如

689
00:30:48,833 --> 00:30:53,566
[音乐]，

690
00:30:53,566 --> 00:30:55,633
完成后我们

691
00:30:55,666 --> 00:30:57,000
只需确保我们位于

692
00:30:57,033 --> 00:31:00,666
此处的基线目录中，然后我们可以运行

693
00:31:00,666 --> 00:31:02,933
预先训练的模型脚本 将

694
00:31:02,933 --> 00:31:07,300
运行预先训练的

695
00:31:07,333 --> 00:31:12,733
交互我们需要一些时间来

696
00:31:12,733 --> 00:31:15,133
启动现在游戏应该打开并且

697
00:31:15,133 --> 00:31:16,966
人工智能将立即开始

698
00:31:16,966 --> 00:31:19,366
在此模式下玩它不会进行任何

699
00:31:19,400 --> 00:31:21,033
额外的学习并且只会根据

700
00:31:21,033 --> 00:31:24,366
它已经

701
00:31:24,400 --> 00:31:28,566
拥有的体验，你可以在人工智能玩游戏的同时与游戏进行实际交互，

702
00:31:28,566 --> 00:31:31,400
例如，我可以

703
00:31:31,433 --> 00:31:32,966
通过使用箭头键来干扰它

704
00:31:32,966 --> 00:31:35,866
并迫使它进入这个

705
00:31:35,900 --> 00:31:38,433
角落，这种模式非常有趣，因为 您

706
00:31:38,466 --> 00:31:40,166
可以将其放入不同的场景中，

707
00:31:40,200 --> 00:31:41,800
看看它如何处理

708
00:31:41,800 --> 00:31:44,533
它们，如果您想完全禁用它，

709
00:31:44,533 --> 00:31:46,233
您可以编辑此文本

710
00:31:46,266 --> 00:31:52,333
文件，因此通过编辑此文本文件代理

711
00:31:52,333 --> 00:31:53,966
启用

712
00:31:53,966 --> 00:31:58,433
txt，如果我们将其更改为否并保存

713
00:31:58,466 --> 00:32:01,000
它，人工智能将

714
00:32:01,033 --> 00:32:03,533
停止使用 任何操作，现在我们可以

715
00:32:03,533 --> 00:32:05,933
完全控制

716
00:32:05,933 --> 00:32:09,666
模拟器，如果我们将此文件编辑回

717
00:32:09,700 --> 00:32:12,100
yes，它将重新激活并继续

718
00:32:12,100 --> 00:32:13,933
播放，如果您想

719
00:32:13,933 --> 00:32:15,366
从头开始训练模型，您将需要

720
00:32:15,400 --> 00:32:17,700
大量的 CPU 核心和内存 但

721
00:32:17,700 --> 00:32:19,700
假设

722
00:32:19,700 --> 00:32:22,133
您要运行的脚本是

723
00:32:22,133 --> 00:32:27,766
基线并行运行，

724
00:32:27,800 --> 00:32:28,966
这将开始运行多个无头

725
00:32:28,966 --> 00:32:32,000
模拟器，没有任何 UI，

726
00:32:32,033 --> 00:32:33,466
这将需要相当长的时间，您

727
00:32:33,466 --> 00:32:34,966
可能需要等待几个小时甚至

728
00:32:34,966 --> 00:32:36,900
几天才能看到积极的

729
00:32:36,933 --> 00:32:39,066
结果，如果 您想要更改

730
00:32:39,100 --> 00:32:40,833
模拟器或游戏的任何基本设置，

731
00:32:40,866 --> 00:32:43,333
此配置可以

732
00:32:43,333 --> 00:32:45,200
在任何运行文件中找到，允许您

733
00:32:45,233 --> 00:32:48,300
执行此操作存储库中的其他文件

734
00:32:48,300 --> 00:32:49,666
允许您对奖励功能进行任何更改

735
00:32:49,700 --> 00:32:52,433
或修改

736
00:32:52,466 --> 00:32:54,500
如果您对代码有任何疑问，请

737
00:32:54,500 --> 00:32:56,200
随时

738
00:32:56,233 --> 00:32:59,133
在 GitHub 上提出问题，

739
00:32:59,133 --> 00:33:00,866
我们已经到了最后，

740
00:33:00,900 --> 00:33:02,933
感谢您的观看，我希望您

741
00:33:02,933 --> 00:33:04,533
对强化

742
00:33:04,533 --> 00:33:06,033
学习，甚至您自己的

743
00:33:06,066 --> 00:33:08,066
心理学有所了解，我真的相信 这些

744
00:33:08,066 --> 00:33:09,833
想法对于理解

745
00:33:09,866 --> 00:33:11,500
我们自己的行为和

746
00:33:11,500 --> 00:33:13,533
推进机器学习一样有用，如果您想

747
00:33:13,533 --> 00:33:18,233
支持这项工作，请参阅描述中的链接，再见

748
00:33:18,266 --> 00:33:26,133
[音乐]，

749
00:33:26,133 --> 00:33:28,133
现在

750
00:33:28,166 --> 00:33:50,466
[音乐]

